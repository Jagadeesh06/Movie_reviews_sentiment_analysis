{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB movie reviews: sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer \n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews(path):\n",
    "    '''\n",
    "    Given the path/to/the/folder, extract the text from all the \n",
    "    files in that folder and return as a dataframe.\n",
    "    '''\n",
    "    filenames = os.listdir(path)\n",
    "    reviews = []\n",
    "    for filename in filenames:\n",
    "        with open(path+filename) as f:\n",
    "            reviews.append(f.read())\n",
    "    df = pd.DataFrame((reviews),columns=['reviews'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Get all the positive and the negative reviews from the files\n",
    "train_pos_df = extract_reviews('train/pos/')\n",
    "train_neg_df = extract_reviews('train/neg/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Label all the reviews with a sentiment label: '1' for positive, and '0' for negative. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  sentiment\n",
       "0  For a movie that gets no respect there sure ar...          1\n",
       "1  Bizarre horror movie filled with famous faces ...          1\n",
       "2  A solid, if unremarkable film. Matthau, as Ein...          1\n",
       "3  It's a strange feeling to sit alone in a theat...          1\n",
       "4  You probably all already know this by now, but...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_df['sentiment'] = np.ones(len(train_pos_df), dtype=np.int8)\n",
    "train_pos_df.head()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Working with one of the best Shakespeare sourc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Well...tremors I, the original started off in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ouch! This one was a bit painful to sit throug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>I've seen some crappy movies in my life, but t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\"Carriers\" follows the exploits of two guys an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  sentiment\n",
       "0  Working with one of the best Shakespeare sourc...          0\n",
       "1  Well...tremors I, the original started off in ...          0\n",
       "2  Ouch! This one was a bit painful to sit throug...          0\n",
       "3  I've seen some crappy movies in my life, but t...          0\n",
       "4  \"Carriers\" follows the exploits of two guys an...          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neg_df['sentiment'] = np.zeros(len(train_neg_df), dtype=np.int8)\n",
    "train_neg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([train_pos_df, train_neg_df],axis=0, ignore_index=True)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shuffle the rows of the dataframe so that there is a random mix of postive and negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Great little thriller. I was expecting some ty...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Nothing could have saved this movie, not even ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>This was a good movie. It wasn't your typical ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>From the pen of Richard Condon (The Manchurian...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>I suppose that today this film has relevance b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  sentiment\n",
       "0  Great little thriller. I was expecting some ty...          1\n",
       "1  Nothing could have saved this movie, not even ...          0\n",
       "2  This was a good movie. It wasn't your typical ...          1\n",
       "3  From the pen of Richard Condon (The Manchurian...          0\n",
       "4  I suppose that today this film has relevance b...          0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the above steps for the test dataset\n",
    "\n",
    "test_pos_df = extract_reviews('test/pos/')\n",
    "test_neg_df = extract_reviews('test/neg/')\n",
    "\n",
    "test_pos_df['sentiment'] = np.ones(len(test_pos_df), dtype=np.int8)\n",
    "test_neg_df['sentiment'] = np.zeros(len(test_neg_df), dtype=np.int8)\n",
    "\n",
    "test_df = pd.concat([test_pos_df, test_neg_df],axis=0, ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Yul Brynner was a symbol of villein in the tin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>This show has been performed live around the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>To sum this story up in a few sentences: A tee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>This is absolutely beyond question the worst m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A box with a button provides a couple with the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  Yul Brynner was a symbol of villein in the tin...\n",
       "1  This show has been performed live around the c...\n",
       "2  To sum this story up in a few sentences: A tee...\n",
       "3  This is absolutely beyond question the worst m...\n",
       "4  A box with a button provides a couple with the..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = test_df.sentiment\n",
    "test_df = test_df.drop('sentiment',axis=1)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = train_df.reviews\n",
    "test_reviews = test_df.reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning\n",
    "- Remove stopwords\n",
    "- Convert to lowercase and remove numbers and punctuations\n",
    "- Keep only those words that are 3 characters or more in length\n",
    "- Lemmatize the words with their appropriate part-of-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'([a-zA-Z]+)') \n",
    "\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess_text(review):\n",
    "    '''\n",
    "    This function preprocesses the comments and sets them up for vectorization.\n",
    "    Input: comment string\n",
    "    Returns: A string after converting the words to lowercase, removing punctuations, and lemmatizing each word\n",
    "    '''\n",
    "    words = [word for word in tokenizer.tokenize(review.lower()) if not word in stop_words]  # convert to lowercase and remove stopwords\n",
    "    clean_words = [word for word in words if len(word)>2]\n",
    "    lemmatized_review = ' '.join([lemmatizer.lemmatize(word,pos= get_wordnet_pos(word)) for word in clean_words]) ## lemmatization\n",
    "    return lemmatized_review  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing steps to the train and test reviews\n",
    "train_reviews = train_reviews.apply(lambda review: preprocess_text(review))\n",
    "test_reviews = test_reviews.apply(lambda review: preprocess_text(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', min_df=5, max_df = 0.6, \n",
    "                                   ngram_range=(1, 2), sublinear_tf=True, max_features=40000)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = tfidf_vectorizer.fit_transform(train_reviews)\n",
    "test_features = tfidf_vectorizer.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 40000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top tokens by tf-idf scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>tf-idf score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11543</td>\n",
       "      <td>film</td>\n",
       "      <td>659.847469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24825</td>\n",
       "      <td>one</td>\n",
       "      <td>502.691351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20672</td>\n",
       "      <td>make</td>\n",
       "      <td>467.155687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30678</td>\n",
       "      <td>see</td>\n",
       "      <td>459.564831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19281</td>\n",
       "      <td>like</td>\n",
       "      <td>459.401386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token  tf-idf score\n",
       "11543  film    659.847469\n",
       "24825   one    502.691351\n",
       "20672  make    467.155687\n",
       "30678   see    459.564831\n",
       "19281  like    459.401386"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_scores = np.sum(train_features.A, axis=0,keepdims=False)\n",
    "p=[]\n",
    "for tag, tfidf_score in zip(feature_names, tfidf_scores):\n",
    "    p.append((tag, tfidf_score))\n",
    "    \n",
    "tfidf_scores_df = pd.DataFrame(p,columns=['token', 'tf-idf score']).sort_values(by = 'tf-idf score', ascending=False)\n",
    "tfidf_scores_df.head() ## top 5 tokens in the corpus by tf-idf score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing uunexpected there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(clf, parameters, X, y, n_jobs=-1, n_folds=5, score_func=None,verbose=0):\n",
    "    if score_func:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func,verbose =verbose)\n",
    "    else:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=5, verbose =verbose)\n",
    "    gs.fit(X, y)\n",
    "    print (\"Best parameter values: {} and best score = {}\".format(gs.best_params_ , gs.best_score_))\n",
    "    best = gs.best_estimator_\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter values: {'alpha': 0.6} and best score = 0.88116\n"
     ]
    }
   ],
   "source": [
    "clf_mulNB = MultinomialNB()\n",
    "parameters = {'alpha': np.arange(0.1, 0.8,0.1)}\n",
    "mulNB_model = grid_search(clf_mulNB, parameters, train_features, y_train, n_folds=5, score_func='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mulNB_pred = mulNB_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(y_test,y_pred):\n",
    "    print(classification_report(y_test, y_pred ))\n",
    "    display(pd.DataFrame(confusion_matrix(y_test, y_pred), \n",
    "                         columns= ['Predicted -ve', 'Predicted +ve'], index = ['Actual -ve', 'Actual +ve']))\n",
    "    print('The AUC (under ROC curve) score is {}'.format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86     12500\n",
      "           1       0.87      0.84      0.86     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted -ve</th>\n",
       "      <th>Predicted +ve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual -ve</td>\n",
       "      <td>10960</td>\n",
       "      <td>1540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual +ve</td>\n",
       "      <td>1969</td>\n",
       "      <td>10531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predicted -ve  Predicted +ve\n",
       "Actual -ve          10960           1540\n",
       "Actual +ve           1969          10531"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC (under ROC curve) score is 0.8596400000000001\n"
     ]
    }
   ],
   "source": [
    "show_metrics(y_test, clf_mulNB_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter values: {'C': 6} and best score = 0.89984\n"
     ]
    }
   ],
   "source": [
    "clf_logreg = LogisticRegression(solver='sag',random_state=42)\n",
    "parameters = {'C': np.arange(1,11,1)}\n",
    "logreg_model = grid_search(clf_logreg, parameters, train_features, y_train, n_folds=10, score_func='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89     12500\n",
      "           1       0.89      0.89      0.89     12500\n",
      "\n",
      "    accuracy                           0.89     25000\n",
      "   macro avg       0.89      0.89      0.89     25000\n",
      "weighted avg       0.89      0.89      0.89     25000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted -ve</th>\n",
       "      <th>Predicted +ve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual -ve</td>\n",
       "      <td>11169</td>\n",
       "      <td>1331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual +ve</td>\n",
       "      <td>1404</td>\n",
       "      <td>11096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predicted -ve  Predicted +ve\n",
       "Actual -ve          11169           1331\n",
       "Actual +ve           1404          11096"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC (under ROC curve) score is 0.8906\n"
     ]
    }
   ],
   "source": [
    "logreg_pred = logreg_model.predict(test_features)\n",
    "show_metrics(y_test, logreg_pred)\n",
    "#print(accuracy_score(y_test, logreg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The accuracy score on the test set is very consistent with my cross-validation results. This is good news, because it means that the model did not overfit.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter values: {'max_depth': 15, 'n_estimators': 1000} and best score = 0.8484\n"
     ]
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(random_state=42,n_jobs=-1, min_samples_split=3) \n",
    "parameters = {'n_estimators': [500,1000], 'max_depth': [10,15]}\n",
    "rfmodel = grid_search(clf_rf, parameters, train_features, y_train, n_folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85     12500\n",
      "           1       0.83      0.89      0.86     12500\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.85     25000\n",
      "weighted avg       0.85      0.85      0.85     25000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted -ve</th>\n",
       "      <th>Predicted +ve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual -ve</td>\n",
       "      <td>10193</td>\n",
       "      <td>2307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual +ve</td>\n",
       "      <td>1417</td>\n",
       "      <td>11083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predicted -ve  Predicted +ve\n",
       "Actual -ve          10193           2307\n",
       "Actual +ve           1417          11083"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC (under ROC curve) score is 0.8510399999999998\n"
     ]
    }
   ],
   "source": [
    "#clf_rf = RandomForestClassifier(n_estimators=500, max_depth=10, random_state=42,n_jobs=-1, min_samples_split=5) \n",
    "#clf_rf.fit(train_features,y_train)\n",
    "rf_pred = rfmodel.predict(test_features)\n",
    "show_metrics(y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word embeddings (word2vec)\n",
    "\n",
    "The results from the different classifiers are comparable and acceptable. I think it might be possible to crank up the accuracy and other metrics (precision, recall) by tweaking hyperparameters a bit more, or by using other classification algorithms. But now, it's time to experiment with Word2vec, a word embedding algorithm which provided a fresh impetus to the NLP community since the original paper by Mikolov et al. in 2013.\n",
    "\n",
    "Word2vec tries to learn meaning of words based on the context in which it is used. This means that it is more than just a bag-of-words model. The order of the words in which they appear do matter in the case of Word2vec and it is generally a good idea to *not* remove stopwords while learning the word vectors. Therefore, instead of tokenizing at the word level, the text is tokenized into individual sentences (or some variation of it). For a deeper understanding of Word2vec, refer to the Stanford CS224N [lecture](https://www.youtube.com/watch?v=8rXD5-xhemo) by Chris Manning.\n",
    "\n",
    "The following section is inspired by this [Kaggle tutorial](https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-3-more-fun-with-word-vectors) on Word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Bean came home yesterday.', 'Whoo!', 'are you coming?']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "s = 'Mr. Bean came home yesterday. Whoo! are you coming?'\n",
    "sent_tokenize(s) #It detects all kinds of punctuations that are normally used at the end of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def text_to_words(review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    review = BeautifulSoup(review).get_text()  #BeautifulSoup is used just to remove HTML tags from some of the reviews\n",
    "    regex_tokenizer = RegexpTokenizer(r'([a-zA-Z]+)') \n",
    "    words = regex_tokenizer.tokenize(review.lower())\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def review_to_sentences(review, remove_stopwords=False ):\n",
    "    '''\n",
    "    Function to split a review into parsed sentences. \n",
    "    Returns a list of sentences, where each sentence is a list of words.\n",
    "    '''\n",
    "    sentences = sent_tokenize(review.strip())  # splits the paragraph in to sentences\n",
    "    sentences_list = []\n",
    "    for sentence in sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(sentence) > 0:\n",
    "        # Otherwise, call sentence_to_words to get a list of words\n",
    "            sentences_list.append(text_to_words(sentence,remove_stopwords)) # split the sentence into words\n",
    "    return sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debashis/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:294: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/debashis/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:357: UserWarning: \"http://www.happierabroad.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/debashis/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:294: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "for review in train_df[\"reviews\"]:\n",
    "    sentences += review_to_sentences(review)\n",
    "\n",
    "for review in test_df[\"reviews\"]:\n",
    "    sentences += review_to_sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.5775325814882915 mins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debashis/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "import logging\n",
    "import time\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "    \n",
    "# Set parameter values for word2vec model\n",
    "num_features = 500    # Word vector dimensionality                      \n",
    "min_word_count = 30   # Minimum word count                        \n",
    "workers = 4       # Number of threads to run in parallel\n",
    "window = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "start = time.time()\n",
    "model = word2vec.Word2Vec(sentences, workers=workers, \n",
    "            size=num_features, min_count = min_word_count, \n",
    "            window = window, sample = downsampling)\n",
    "\n",
    "#model.init_sims(replace=True)\n",
    "end = time.time()\n",
    "print('Took {} mins.'.format((end - start)/60))\n",
    "# save the model for later use. Can load it later using Word2Vec.load()\n",
    "model_name = \"500features_30minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45675613759447187"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cosine similarity between any two words in the vocabulary\n",
    "model.wv.similarity('movie', 'thriller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drama', 0.7012590169906616),\n",
       " ('flick', 0.6641907095909119),\n",
       " ('giallo', 0.6298017501831055),\n",
       " ('yarn', 0.6260671615600586),\n",
       " ('chiller', 0.6184465885162354),\n",
       " ('melodrama', 0.6034889221191406),\n",
       " ('suspense', 0.5908105373382568),\n",
       " ('farce', 0.5786558389663696),\n",
       " ('whodunit', 0.5651916265487671),\n",
       " ('mystery', 0.5618135929107666)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 similar words\n",
    "model.wv.most_similar('thriller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324.20386"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The vectors are normalized\n",
    "np.sum(model.wv['thriller']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15358"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No. of words in the vocabulary\n",
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15358, 500)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row is the word vector for that word\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we would represent each piece of review with a vector as follows:**\n",
    "1. Loop over each word in the review, if the word is in the vocabulary, get its word vector.\n",
    "2. Sum all the word vectors found in this way and divide by the number of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_feature_vector(model,words):\n",
    "    '''Returns the average of all the word vectors corresponding to a particular review'''\n",
    "    vector_sum = np.zeros(num_features,)\n",
    "    c=0\n",
    "    for word in words:\n",
    "        if word in model.wv.vocab:\n",
    "            vector_sum  = vector_sum + model.wv[word]\n",
    "            c = c+1\n",
    "    vector_sum /= c\n",
    "    return vector_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n",
      "(25000, 500)\n"
     ]
    }
   ],
   "source": [
    "train_wordvec_features = np.empty((train_df.shape[0], num_features))\n",
    "test_wordvec_features = np.empty((test_df.shape[0], num_features))\n",
    "\n",
    "for row in range(train_df.shape[0]):  \n",
    "    train_wordvec_features[row,:] = text_to_feature_vector(model, text_to_words(train_df['reviews'][row],remove_stopwords=True))\n",
    "\n",
    "for row in range(test_df.shape[0]):\n",
    "    test_wordvec_features[row,:] = text_to_feature_vector(model, text_to_words(test_df['reviews'][row], remove_stopwords=True))\n",
    "\n",
    "print(train_wordvec_features.shape)\n",
    "print(test_wordvec_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.arange(1,20,1)\n",
    "cv_scores = []\n",
    "for c in C:\n",
    "    clf_logreg_word2vec = LogisticRegression(C=c, solver='lbfgs',random_state=42, max_iter=1000)\n",
    "    cv_score = np.mean(cross_val_score(clf_logreg_word2vec, train_wordvec_features,y_train,cv=5))\n",
    "    cv_scores.append(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_  = plt.plot(C,cv_scores, 'ro-')\n",
    "_=plt.xlabel('C: regularization parameter (= 1/$\\lambda$)')\n",
    "_= plt.ylabel('Mean cross-validation score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debashis/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_logreg_word2vec = LogisticRegression(C=10, random_state=42, max_iter=1000)\n",
    "clf_logreg_word2vec.fit(train_wordvec_features, y_train)\n",
    "logreg_pred_word2vec  = clf_logreg_word2vec.predict(test_wordvec_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87     12500\n",
      "           1       0.87      0.87      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted -ve</th>\n",
       "      <th>Predicted +ve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual -ve</td>\n",
       "      <td>10860</td>\n",
       "      <td>1640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual +ve</td>\n",
       "      <td>1634</td>\n",
       "      <td>10866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predicted -ve  Predicted +ve\n",
       "Actual -ve          10860           1640\n",
       "Actual +ve           1634          10866"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC (under ROC curve) score is 0.86904\n",
      "0.86904\n"
     ]
    }
   ],
   "source": [
    "show_metrics(y_test,logreg_pred_word2vec)\n",
    "print(accuracy_score(y_test,logreg_pred_word2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mis-classification example: Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_misclf = np.where(~(logreg_pred_word2vec == y_test))[0][0]\n",
    "ind_misclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 0\n",
      "Actual: 1\n"
     ]
    }
   ],
   "source": [
    "print('Predicted: %i'%logreg_pred_word2vec[ind_misclf])\n",
    "print('Actual: %i'%y_test[ind_misclf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To sum this story up in a few sentences: A teenage girl (Amy) uses her hot body and \"supposed\" virginity to entice a young troubled guy (Matt) with a potential football scholarship to provide her a \"Full Ride\" out of town. Come to find out she has quite the reputation & has slept with many football players in the past hoping they would offer her the same deal. Both of these kids have come from troubled & dysfunctional homes. Matt's mothers a alcoholic who repeatedly embarrasses him in front of his friends & Amy's mother had a bad reputation herself & got pregnant with Amy at a a young age. Matt falls in love with Amy & tries to straighten out his life for her. Very predictable ending. The actress that plays \"Amy\" is actually 33 years old trying to play a teenager!\n"
     ]
    }
   ],
   "source": [
    "print(test_df.iloc[ind_misclf]['reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83     12500\n",
      "           1       0.83      0.83      0.83     12500\n",
      "\n",
      "    accuracy                           0.83     25000\n",
      "   macro avg       0.83      0.83      0.83     25000\n",
      "weighted avg       0.83      0.83      0.83     25000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted -ve</th>\n",
       "      <th>Predicted +ve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual -ve</td>\n",
       "      <td>10314</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual +ve</td>\n",
       "      <td>2150</td>\n",
       "      <td>10350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predicted -ve  Predicted +ve\n",
       "Actual -ve          10314           2186\n",
       "Actual +ve           2150          10350"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC (under ROC curve) score is 0.8265599999999999\n"
     ]
    }
   ],
   "source": [
    "clf_rf_word2vec = RandomForestClassifier(n_estimators=500, max_depth=10, random_state=42,n_jobs=-1, min_samples_split=3) \n",
    "clf_rf_word2vec.fit(train_wordvec_features,y_train)\n",
    "rf_pred_word2vec = clf_rf_word2vec.predict(test_wordvec_features)\n",
    "\n",
    "show_metrics(y_test,rf_pred_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter values: {'max_depth': 15, 'n_estimators': 500} and best score = 0.83272\n"
     ]
    }
   ],
   "source": [
    "clf_rf_word2vec = RandomForestClassifier(random_state=42,n_jobs=-1, min_samples_split=3) \n",
    "parameters = {'n_estimators': [200,500,700], 'max_depth': [10,15]}\n",
    "\n",
    "rfmodel_word2vec = grid_search(clf_rf_word2vec, parameters, train_wordvec_features, y_train, n_folds=10)\n",
    "rf_pred_word2vec = rfmodel_word2vec.predict(test_wordvec_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec conclusion\n",
    "\n",
    "We didn't get any performance improvement by using word embeddings in this particular problem. In fact, the accuracy scores dropped by a few per cent! \n",
    "- Why would that be?\n",
    "Maybe the answer lies in the averaging process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification in a reduced featured space (LSA)\n",
    "\n",
    "If you have ever heard about Pricnipal Componenet Analysis, that is conceptually the same thing as Latent Semantic Analysis, or, LSA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "\n",
    "train_svd = svd.fit_transform(train_features)\n",
    "test_svd = svd.transform(test_features)\n",
    "print(train_svd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 40000)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter values: {'C': 9} and best score = 0.86848\n"
     ]
    }
   ],
   "source": [
    "logreg_svd = LogisticRegression(solver = 'lbfgs',random_state=42)\n",
    "parameters = {'C':np.arange(1,20,1)}\n",
    "logreg_svd = grid_search(logreg_svd, parameters, train_svd, y_train, n_folds=10, score_func='accuracy')\n",
    "logreg_pred_svd  = logreg_svd.predict(test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.87     12500\n",
      "           1       0.87      0.87      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted -ve</th>\n",
       "      <th>Predicted +ve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual -ve</td>\n",
       "      <td>10799</td>\n",
       "      <td>1701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual +ve</td>\n",
       "      <td>1580</td>\n",
       "      <td>10920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predicted -ve  Predicted +ve\n",
       "Actual -ve          10799           1701\n",
       "Actual +ve           1580          10920"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC (under ROC curve) score is 0.8687600000000001\n"
     ]
    }
   ],
   "source": [
    "show_metrics(y_test,logreg_pred_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __*This is super cool! We have reduced the number of dimensions (features) from 40000 to 100, and still managed to achieve an accuracy that is just 2% less.*__\n",
    "\n",
    "If I were low on resources and time, I would prefer reducing the dimensionality and then perform the classification, unless it is for a competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nteract": {
   "version": "0.14.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
